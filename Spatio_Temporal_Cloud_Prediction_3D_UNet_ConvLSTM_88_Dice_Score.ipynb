{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models\n!pip install h5py boto3 matplotlib numpy opencv-python\n# !pip install -q tensorflow-addons","metadata":{"id":"pZn-q7qleBpD","outputId":"62bce1bd-ddbb-43d2-d29f-955da252f81e","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-06-16T05:24:22.052221Z","iopub.execute_input":"2025-06-16T05:24:22.052748Z","iopub.status.idle":"2025-06-16T05:24:25.268011Z","shell.execute_reply.started":"2025-06-16T05:24:22.052727Z","shell.execute_reply":"2025-06-16T05:24:25.267000Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.38.11)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: botocore<1.39.0,>=1.38.11 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.38.11)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\nRequirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.12.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.11->boto3) (2.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\nimport boto3\nfrom botocore.handlers import disable_signing\nimport os\n\n# Create an S3 client with anonymous access\ns3 = boto3.resource('s3')\ns3.meta.client.meta.events.register('choose-signer.s3.*', disable_signing)\n\nbucket = s3.Bucket('sevir')\nprefix = 'data/'\n\n# Create directory to store files\nos.makedirs('sevir_data', exist_ok=True)\n\n# # List available files\n# print(\"📦 Listing available .h5 files...\")\n# for obj in bucket.objects.filter(Prefix=prefix):\n#     if obj.key.endswith('.h5'):\n#         print(obj.key)","metadata":{"id":"ymQc_uFnd_wb","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:24:25.269849Z","iopub.execute_input":"2025-06-16T05:24:25.270103Z","iopub.status.idle":"2025-06-16T05:24:27.329118Z","shell.execute_reply.started":"2025-06-16T05:24:25.270079Z","shell.execute_reply":"2025-06-16T05:24:27.328360Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport boto3\nfrom botocore.handlers import disable_signing\n\n# Connect anonymously to the SEVIR S3 bucket\ns3 = boto3.resource('s3')\ns3.meta.client.meta.events.register('choose-signer.s3.*', disable_signing)\nbucket = s3.Bucket('sevir')\n\n# Delete all files in a folder\ndef clear_folder(folder_path):\n    if not os.path.exists(folder_path):\n        return\n    for f in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, f)\n        os.remove(file_path)\n    print(f\"🧹 Cleared folder: {folder_path}\")\n\n# Download a single .h5 file\ndef download_one_ir107():\n    prefix = 'data/ir107/'\n    local_folder = 'SEVIR_data/ir107'\n    os.makedirs(local_folder, exist_ok=True)\n\n    print(f\"\\n📥 Searching for one .h5 file in {prefix} ...\")\n    for obj in bucket.objects.filter(Prefix=prefix):\n        if obj.key.endswith('.h5'):\n            filename = os.path.basename(obj.key)\n            local_path = os.path.join(local_folder, filename)\n            size_mb = obj.size / (1024 * 1024)\n            print(f\"➡️ Downloading {filename} ({size_mb:.2f} MB)...\")\n            bucket.download_file(obj.key, local_path)\n            break  # Only download one file\n\n# STEP 1: Clear ir069 and ir107 folders\nclear_folder('SEVIR_data/ir069')\nclear_folder('SEVIR_data/ir107')\n\n# STEP 2: Download one file from ir107\ndownload_one_ir107()\n\nprint(\"\\n✅ Done: One ir107 file downloaded. ir069 and ir107 folders cleared before download.\")\n","metadata":{"id":"BdevMdtueFr0","outputId":"e19ae9a4-9914-4369-8598-dcd4144bbb0b","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-06-16T05:24:27.329895Z","iopub.execute_input":"2025-06-16T05:24:27.330154Z","iopub.status.idle":"2025-06-16T05:24:42.663165Z","shell.execute_reply.started":"2025-06-16T05:24:27.330129Z","shell.execute_reply":"2025-06-16T05:24:42.662346Z"}},"outputs":[{"name":"stdout","text":"🧹 Cleared folder: SEVIR_data/ir107\n\n📥 Searching for one .h5 file in data/ir107/ ...\n➡️ Downloading SEVIR_IR107_RANDOMEVENTS_2018_0101_0430.h5 (1905.27 MB)...\n\n✅ Done: One ir107 file downloaded. ir069 and ir107 folders cleared before download.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import h5py\nimport numpy as np\n\n# file_path = '/content/SEVIR_data/ir107/SEVIR_IR107_RANDOMEVENTS_2018_0101_0430.h5'\nfile_path = '/kaggle/working/SEVIR_data/ir107/SEVIR_IR107_RANDOMEVENTS_2018_0101_0430.h5'\n\nwith h5py.File(file_path, 'r') as f:\n    print(\"Top-level keys:\", list(f.keys()))\n    data = f['ir107'][:]  # Load entire ir107 dataset into memory\n\n# Transpose to (event, time, height, width) => (553, 49, 192, 192)\ndata = data.transpose(0, 3, 1, 2)\nprint(\"Transposed shape:\", data.shape)\n","metadata":{"id":"NYUFFxbfeGfE","outputId":"03b572c8-f1ce-4aa3-e64c-d5b213e425a1","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-06-16T05:24:42.665004Z","iopub.execute_input":"2025-06-16T05:24:42.665338Z","iopub.status.idle":"2025-06-16T05:24:44.189391Z","shell.execute_reply.started":"2025-06-16T05:24:42.665319Z","shell.execute_reply":"2025-06-16T05:24:44.188483Z"}},"outputs":[{"name":"stdout","text":"Top-level keys: ['id', 'ir107']\nTransposed shape: (553, 49, 192, 192)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# with h5py.File(file_path, 'r') as f:\n#     def plot_event(sequence):\n#         def normalize(frame):\n#             return np.clip((frame - 180) / (300 - 180), 0, 1)\n#         plt.figure(figsize=(15, 3))\n#         for i, idx in enumerate([0, 8, 16, 24, 32, 40, 48]):\n#             norm_frame = normalize(sequence[idx])\n#             plt.subplot(1, 7, i + 1)\n#             plt.imshow(norm_frame, cmap='inferno')\n#             plt.title(f'Frame {idx}')\n#             plt.axis('off')\n#         plt.tight_layout()\n#         plt.show()\n\n#     for i in range(10):\n#         print(f\"🌀 Event {i}\")\n#         plot_event(f['ir107'][i])","metadata":{"id":"fZ84wTwTeKA8","outputId":"9c40fab5-0f4f-4687-e694-904c66c6553c","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:24:44.190089Z","iopub.execute_input":"2025-06-16T05:24:44.190342Z","iopub.status.idle":"2025-06-16T05:24:44.194157Z","shell.execute_reply.started":"2025-06-16T05:24:44.190324Z","shell.execute_reply":"2025-06-16T05:24:44.193479Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport gc\n\ndef simple_memory_efficient_preprocessing(data):\n    \"\"\"Simplified preprocessing that won't crash your notebook\"\"\"\n    print(f\"Input data shape: {data.shape}\")\n    print(f\"Data type: {data.dtype}\")\n    print(f\"Temperature range: [{data.min():.1f}, {data.max():.1f}] K\")\n    \n    # Memory management\n    gc.collect()\n    \n    # Simple normalization\n    def normalize(frames):\n        return np.clip((frames - 180) / (300 - 180), 0, 1)\n    \n    # Simple activity check\n    def has_activity(x, threshold=0.01):\n        return np.var(x) > threshold\n    \n    X_inputs, Y_outputs = [], []\n    \n    for i, event in enumerate(data):\n        if i % 50 == 0:\n            print(f\"Processing event {i}/{len(data)}\")\n            gc.collect()  # Force garbage collection every 50 events\n        \n        try:\n            # Normalize\n            norm_event = normalize(event.astype(np.float32))\n            \n            # Create single sequence (no overlapping to save memory)\n            if norm_event.shape[0] >= 18:\n                input_seq = norm_event[:12]      # First 12 frames\n                target_seq = norm_event[12:18]   # Next 6 frames\n                \n                # Check activity\n                if has_activity(input_seq) and has_activity(target_seq):\n                    X_inputs.append(input_seq)\n                    Y_outputs.append(target_seq)\n        except Exception as e:\n            print(f\"Error processing event {i}: {e}\")\n            continue\n    \n    # Convert to arrays with memory management\n    print(\"Converting to arrays...\")\n    X_inputs = np.array(X_inputs, dtype=np.float32)\n    Y_outputs = np.array(Y_outputs, dtype=np.float32)\n    \n    print(f\"✅ Preprocessing complete!\")\n    print(f\"Valid sequences: {len(X_inputs)}\")\n    print(f\"Input shape: {X_inputs.shape}\")\n    print(f\"Output shape: {Y_outputs.shape}\")\n    print(f\"Memory usage: {(X_inputs.nbytes + Y_outputs.nbytes) / 1e9:.2f} GB\")\n    \n    # Simple train/val split\n    X_train, X_val, Y_train, Y_val = train_test_split(\n        X_inputs, Y_outputs, test_size=0.2, random_state=42\n    )\n    \n    # Add channel dimension: (N, T, H, W, 1)\n    X_train = X_train[..., np.newaxis]\n    X_val = X_val[..., np.newaxis]\n    Y_train = Y_train[..., np.newaxis]\n    Y_val = Y_val[..., np.newaxis]\n    \n    print(f\"\\n📊 Data Split Summary:\")\n    print(f\"Train: {X_train.shape[0]} samples\")\n    print(f\"Val:   {X_val.shape[0]} samples\")\n    print(f\"Final shapes: X{X_train.shape}, Y{Y_train.shape}\")\n    \n    # Final memory cleanup\n    del X_inputs, Y_outputs\n    gc.collect()\n    \n    return X_train, X_val, Y_train, Y_val\n\n# Memory management settings\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\n# Limit TensorFlow memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Run preprocessing\nX_train, X_val, Y_train, Y_val = simple_memory_efficient_preprocessing(data)\n","metadata":{"id":"nanDJZlAyStM","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:24:44.195080Z","iopub.execute_input":"2025-06-16T05:24:44.195334Z","iopub.status.idle":"2025-06-16T05:24:56.875112Z","shell.execute_reply.started":"2025-06-16T05:24:44.195309Z","shell.execute_reply":"2025-06-16T05:24:56.874349Z"}},"outputs":[{"name":"stdout","text":"Input data shape: (553, 49, 192, 192)\nData type: int16\nTemperature range: [-32768.0, 4590.0] K\nProcessing event 0/553\nProcessing event 50/553\nProcessing event 100/553\nProcessing event 150/553\nProcessing event 200/553\nProcessing event 250/553\nProcessing event 300/553\nProcessing event 350/553\nProcessing event 400/553\nProcessing event 450/553\nProcessing event 500/553\nProcessing event 550/553\nConverting to arrays...\n✅ Preprocessing complete!\nValid sequences: 259\nInput shape: (259, 12, 192, 192)\nOutput shape: (259, 6, 192, 192)\nMemory usage: 0.69 GB\n\n📊 Data Split Summary:\nTrain: 207 samples\nVal:   52 samples\nFinal shapes: X(207, 12, 192, 192, 1), Y(207, 6, 192, 192, 1)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# (X_train, X_val, X_test), (Y_train, Y_val, Y_test) = main_preprocessing_pipeline(data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksijDWNmyzHh","outputId":"e7320348-a477-4694-e2df-7bd34cb3b472","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:24:56.875935Z","iopub.execute_input":"2025-06-16T05:24:56.876241Z","iopub.status.idle":"2025-06-16T05:24:56.879945Z","shell.execute_reply.started":"2025-06-16T05:24:56.876200Z","shell.execute_reply":"2025-06-16T05:24:56.879243Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import os\nos.environ['TF_ENABLE_LAYOUT_OPTIMIZER'] = '0'\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# Simplified constants\nL2_REG = 1e-5  # Reduced regularization\nDROPOUT_RATE = 0.2  # Reduced dropout\n\ndef conv3d_block(x, filters, kernel_regularizer=None, dropout_rate=0.0):\n    x = layers.Conv3D(filters, (3, 3, 3), padding='same', kernel_regularizer=kernel_regularizer)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    if dropout_rate > 0:\n        x = layers.Dropout(dropout_rate)(x)\n    x = layers.Conv3D(filters, (3, 3, 3), padding='same', kernel_regularizer=kernel_regularizer)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    return x\n\ndef simple_decoder3d_block(x, skip, filters, kernel_regularizer=None, dropout_rate=0.0):\n    x = layers.UpSampling3D(size=(1, 2, 2))(x)\n    x = layers.Conv3D(filters, (3, 3, 3), padding='same', kernel_regularizer=kernel_regularizer)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Concatenate()([x, skip])\n    x = conv3d_block(x, filters, kernel_regularizer, dropout_rate)\n    return x\n\ndef build_simple_effective_model(input_shape=(12, 192, 192, 1)):\n    inputs = Input(shape=input_shape)\n\n    # Simpler encoder\n    e1 = conv3d_block(inputs, 16, l2(L2_REG), DROPOUT_RATE * 0.5)\n    p1 = layers.MaxPooling3D((1, 2, 2))(e1)\n\n    e2 = conv3d_block(p1, 32, l2(L2_REG), DROPOUT_RATE * 0.7)\n    p2 = layers.MaxPooling3D((1, 2, 2))(e2)\n\n    e3 = conv3d_block(p2, 64, l2(L2_REG), DROPOUT_RATE)\n    p3 = layers.MaxPooling3D((1, 2, 2))(e3)\n\n    # Simple ConvLSTM bottleneck (single direction)\n    b = layers.Reshape((12, 24, 24, 64))(p3)\n    b = layers.ConvLSTM2D(96, (3, 3), padding=\"same\", return_sequences=True,\n                          activation='relu', kernel_regularizer=l2(L2_REG))(b)\n    b = layers.BatchNormalization()(b)\n    b = layers.Dropout(DROPOUT_RATE)(b)\n    b = layers.Reshape((12, 24, 24, 96))(b)\n\n    # Simple decoder\n    d1 = simple_decoder3d_block(b, e3, 64, l2(L2_REG), DROPOUT_RATE * 0.6)\n    d2 = simple_decoder3d_block(d1, e2, 32, l2(L2_REG), DROPOUT_RATE * 0.4)\n    d3 = simple_decoder3d_block(d2, e1, 16, l2(L2_REG), DROPOUT_RATE * 0.2)\n\n    # Output layer\n    output_conv = layers.Conv3D(1, (1, 1, 1), padding='same', activation='sigmoid')(d3)\n    outputs = layers.Lambda(lambda t: t[:, -6:, :, :, :])(output_conv)\n\n    return models.Model(inputs, outputs)\n\n# FIXED: Simple, effective loss with proper type casting\ndef dice_coefficient(y_true, y_pred, smooth=1e-6):\n    # CRITICAL FIX: Cast both tensors to float32 to avoid type mismatch\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\ndef simple_combined_loss(y_true, y_pred):\n    # CRITICAL FIX: Cast both tensors to float32 at the start\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    # Just focal + dice, no connectivity loss\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    dice_loss = 1 - dice_coefficient(y_true, y_pred)\n    return 0.6 * tf.reduce_mean(bce) + 0.4 * dice_loss\n\n# Build and compile\nmodel = build_simple_effective_model()\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-4),  # Higher learning rate\n    loss=simple_combined_loss,\n    metrics=[dice_coefficient]\n)\n\n# Simple callbacks\ncallbacks = [\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n]\n\nprint(\"Simple, effective model ready for training!\")\n# model.summary()\n","metadata":{"id":"_Wb3zsw1eZM0","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:24:56.880821Z","iopub.execute_input":"2025-06-16T05:24:56.881631Z","iopub.status.idle":"2025-06-16T05:24:57.204238Z","shell.execute_reply.started":"2025-06-16T05:24:56.881606Z","shell.execute_reply":"2025-06-16T05:24:57.203500Z"}},"outputs":[{"name":"stdout","text":"Simple, effective model ready for training!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"X_train shape:\", X_val.shape)\nprint(\"Y_train shape:\", Y_val.shape)\n","metadata":{"id":"kKzGP7xEebH4","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:24:57.205042Z","iopub.execute_input":"2025-06-16T05:24:57.205314Z","iopub.status.idle":"2025-06-16T05:24:57.209927Z","shell.execute_reply.started":"2025-06-16T05:24:57.205295Z","shell.execute_reply":"2025-06-16T05:24:57.209029Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (52, 12, 192, 192, 1)\nY_train shape: (52, 6, 192, 192, 1)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- Training Hyperparameters ---\nEPOCHS = 30\nBATCH_SIZE = 8  # Reduced for memory efficiency with your model\n\n# --- Start Training ---\nprint(\"Starting model training...\")\n\nhistory = model.fit(\n    x=X_train,\n    y=Y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val, Y_val),\n    callbacks=callbacks,  # Use the callbacks defined in your model code\n    verbose=1\n)\n\nprint(\"\\nTraining finished.\")\nprint(\"Model saved as 'simple_cloud_model.h5'\")\n\n# Analyze results immediately after training\nplot_training_history(history)\n","metadata":{"id":"hsrZEd-cedqF","trusted":true,"outputId":"a021704e-296f-4157-a39f-9b1089834876","execution":{"iopub.status.busy":"2025-06-16T05:24:57.212026Z","iopub.execute_input":"2025-06-16T05:24:57.212304Z","iopub.status.idle":"2025-06-16T05:49:24.793424Z","shell.execute_reply.started":"2025-06-16T05:24:57.212279Z","shell.execute_reply":"2025-06-16T05:49:24.792347Z"}},"outputs":[{"name":"stdout","text":"Starting model training...\nEpoch 1/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3s/step - dice_coefficient: 0.5535 - loss: 0.4622 - val_dice_coefficient: 0.3879 - val_loss: 0.6425 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - dice_coefficient: 0.7783 - loss: 0.2393 - val_dice_coefficient: 0.3940 - val_loss: 0.6300 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - dice_coefficient: 0.8053 - loss: 0.2004 - val_dice_coefficient: 0.4056 - val_loss: 0.6140 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8241 - loss: 0.1797 - val_dice_coefficient: 0.4117 - val_loss: 0.5978 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8386 - loss: 0.1721 - val_dice_coefficient: 0.4444 - val_loss: 0.5967 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8391 - loss: 0.1686 - val_dice_coefficient: 0.4573 - val_loss: 0.5778 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8581 - loss: 0.1580 - val_dice_coefficient: 0.4563 - val_loss: 0.5895 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8533 - loss: 0.1528 - val_dice_coefficient: 0.4811 - val_loss: 0.5240 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8764 - loss: 0.1403 - val_dice_coefficient: 0.5143 - val_loss: 0.4720 - learning_rate: 1.0000e-04\nEpoch 10/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8737 - loss: 0.1404 - val_dice_coefficient: 0.5395 - val_loss: 0.4550 - learning_rate: 1.0000e-04\nEpoch 11/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8612 - loss: 0.1479 - val_dice_coefficient: 0.5060 - val_loss: 0.4893 - learning_rate: 1.0000e-04\nEpoch 12/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8164 - loss: 0.1680 - val_dice_coefficient: 0.5987 - val_loss: 0.3747 - learning_rate: 1.0000e-04\nEpoch 13/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8720 - loss: 0.1399 - val_dice_coefficient: 0.5975 - val_loss: 0.3725 - learning_rate: 1.0000e-04\nEpoch 14/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8726 - loss: 0.1412 - val_dice_coefficient: 0.6862 - val_loss: 0.2948 - learning_rate: 1.0000e-04\nEpoch 15/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8380 - loss: 0.1568 - val_dice_coefficient: 0.7204 - val_loss: 0.2638 - learning_rate: 1.0000e-04\nEpoch 16/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8695 - loss: 0.1390 - val_dice_coefficient: 0.7227 - val_loss: 0.2659 - learning_rate: 1.0000e-04\nEpoch 17/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8929 - loss: 0.1306 - val_dice_coefficient: 0.7773 - val_loss: 0.2184 - learning_rate: 1.0000e-04\nEpoch 18/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8707 - loss: 0.1364 - val_dice_coefficient: 0.7991 - val_loss: 0.1999 - learning_rate: 1.0000e-04\nEpoch 19/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8866 - loss: 0.1301 - val_dice_coefficient: 0.7882 - val_loss: 0.2157 - learning_rate: 1.0000e-04\nEpoch 20/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8842 - loss: 0.1303 - val_dice_coefficient: 0.8383 - val_loss: 0.1695 - learning_rate: 1.0000e-04\nEpoch 21/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8747 - loss: 0.1365 - val_dice_coefficient: 0.8366 - val_loss: 0.1740 - learning_rate: 1.0000e-04\nEpoch 22/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8719 - loss: 0.1390 - val_dice_coefficient: 0.8544 - val_loss: 0.1659 - learning_rate: 1.0000e-04\nEpoch 23/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8892 - loss: 0.1271 - val_dice_coefficient: 0.8686 - val_loss: 0.1517 - learning_rate: 1.0000e-04\nEpoch 24/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8697 - loss: 0.1338 - val_dice_coefficient: 0.8691 - val_loss: 0.1514 - learning_rate: 1.0000e-04\nEpoch 25/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8973 - loss: 0.1232 - val_dice_coefficient: 0.8617 - val_loss: 0.1537 - learning_rate: 1.0000e-04\nEpoch 26/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8749 - loss: 0.1324 - val_dice_coefficient: 0.8718 - val_loss: 0.1514 - learning_rate: 1.0000e-04\nEpoch 27/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8741 - loss: 0.1316 - val_dice_coefficient: 0.8777 - val_loss: 0.1395 - learning_rate: 1.0000e-04\nEpoch 28/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8875 - loss: 0.1279 - val_dice_coefficient: 0.8782 - val_loss: 0.1389 - learning_rate: 1.0000e-04\nEpoch 29/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8893 - loss: 0.1225 - val_dice_coefficient: 0.8779 - val_loss: 0.1432 - learning_rate: 1.0000e-04\nEpoch 30/30\n\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - dice_coefficient: 0.8971 - loss: 0.1216 - val_dice_coefficient: 0.8857 - val_loss: 0.1394 - learning_rate: 1.0000e-04\nRestoring model weights from the end of the best epoch: 28.\n\nTraining finished.\nModel saved as 'simple_cloud_model.h5'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3616207203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Analyze results immediately after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'plot_training_history' is not defined"],"ename":"NameError","evalue":"name 'plot_training_history' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"model.save(\"cloud_predictor_model.h5\")\n","metadata":{"id":"SEHix7rYepgw","trusted":true,"execution":{"execution_failed":"2025-06-16T07:51:34.149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nfrom scipy.ndimage import gaussian_filter\nfrom skimage import morphology\n\ndef plot_training_history(history):\n    \"\"\"Enhanced training history plotting\"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Loss\n    ax1.plot(history.history['loss'], label='Train Loss', linewidth=2)\n    ax1.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title(\"Training vs Validation Loss\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Dice Score\n    ax2.plot(history.history['dice_coefficient'], label='Train Dice', linewidth=2)\n    ax2.plot(history.history['val_dice_coefficient'], label='Val Dice', linewidth=2)\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Dice Coefficient')\n    ax2.set_title('Dice Score (Train vs Val)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Learning Rate\n    if 'lr' in history.history:\n        ax3.plot(history.history['lr'], linewidth=2)\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('Learning Rate')\n        ax3.set_title('Learning Rate Schedule')\n        ax3.set_yscale('log')\n        ax3.grid(True, alpha=0.3)\n    else:\n        ax3.text(0.5, 0.5, 'Learning Rate\\nNot Available', ha='center', va='center', transform=ax3.transAxes)\n        ax3.set_title('Learning Rate Schedule')\n    \n    # Overfitting Analysis\n    train_loss = np.array(history.history['loss'])\n    val_loss = np.array(history.history['val_loss'])\n    overfitting_metric = val_loss - train_loss\n    \n    ax4.plot(overfitting_metric, linewidth=2, color='red')\n    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Val Loss - Train Loss')\n    ax4.set_title('Overfitting Indicator')\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef gentle_post_processing(pred, threshold=0.3, min_size=20):\n    \"\"\"Gentle post-processing that preserves predictions\"\"\"\n    pred_squeezed = pred.squeeze()\n    \n    # Convert to float32 if needed\n    if pred_squeezed.dtype == np.float16:\n        pred_squeezed = pred_squeezed.astype(np.float32)\n    \n    out = np.zeros_like(pred_squeezed)\n    \n    for t in range(pred_squeezed.shape[0]):\n        # Lower threshold to keep more predictions\n        binary = pred_squeezed[t] > threshold\n        \n        # Minimal cleanup\n        binary = morphology.remove_small_objects(binary, min_size=min_size)\n        \n        out[t] = binary.astype(np.uint8)\n    \n    return out\n\ndef calculate_metrics(y_true, y_pred):\n    \"\"\"Calculate comprehensive metrics\"\"\"\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    \n    intersection = np.sum(y_true_f * y_pred_f)\n    union = np.sum(y_true_f) + np.sum(y_pred_f) - intersection\n    \n    dice = (2. * intersection) / (np.sum(y_true_f) + np.sum(y_pred_f) + 1e-6)\n    iou = intersection / (union + 1e-6)\n    \n    tp = intersection\n    fp = np.sum(y_pred_f) - intersection\n    fn = np.sum(y_true_f) - intersection\n    \n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n    \n    return {'dice': dice, 'iou': iou, 'precision': precision, 'recall': recall, 'f1': f1}\n\ndef visualize_predictions_enhanced(X_sample, Y_true, Y_pred_raw, Y_pred_processed):\n    \"\"\"Enhanced visualization with metrics\"\"\"\n    fig, axs = plt.subplots(4, 6, figsize=(20, 12))\n    \n    metrics_list = []\n    \n    for i in range(6):\n        # Input frames\n        axs[0, i].imshow(X_sample[i+6].squeeze(), cmap='gray')\n        axs[0, i].set_title(f\"Input t{i+6}\")\n        axs[0, i].axis('off')\n        \n        # Raw predictions\n        axs[1, i].imshow(Y_pred_raw[i].squeeze(), cmap='viridis', vmin=0, vmax=1)\n        axs[1, i].set_title(f\"Raw Pred t{i+12}\")\n        axs[1, i].axis('off')\n        \n        # Processed predictions\n        axs[2, i].imshow(Y_pred_processed[i], cmap='gray')\n        axs[2, i].set_title(f\"Processed t{i+12}\")\n        axs[2, i].axis('off')\n        \n        # Ground truth\n        axs[3, i].imshow(Y_true[i].squeeze(), cmap='gray')\n        axs[3, i].set_title(f\"Ground Truth t{i+12}\")\n        axs[3, i].axis('off')\n        \n        # Calculate metrics for this frame\n        metrics = calculate_metrics(Y_true[i].squeeze(), Y_pred_processed[i])\n        metrics_list.append(metrics)\n    \n    plt.suptitle(\"Cloud Prediction Analysis\", fontsize=16)\n    plt.tight_layout()\n    \n    # Print average metrics\n    avg_metrics = {}\n    for key in metrics_list[0].keys():\n        avg_metrics[key] = np.mean([m[key] for m in metrics_list])\n    \n    print(\"Average Metrics:\")\n    for key, value in avg_metrics.items():\n        print(f\"{key.upper()}: {value:.4f}\")\n    \n    plt.show()\n    \n    return avg_metrics\n\n# Analysis code\ndef analyze_model_results(model, X_val, Y_val, sample_idx=3):\n    \"\"\"Complete analysis function\"\"\"\n    # 1. Plot training history\n    plot_training_history(history)\n    \n    # 2. Choose a validation sample\n    X_sample = X_val[sample_idx]\n    Y_true = Y_val[sample_idx]\n    \n    print(f\"Analyzing sample {sample_idx}...\")\n    \n    # 3. Get model predictions\n    print(\"Getting model predictions...\")\n    Y_pred_raw = model.predict(X_sample[np.newaxis, ...])[0].astype(np.float32)\n    \n    print(f\"Prediction shape: {Y_pred_raw.shape}, dtype: {Y_pred_raw.dtype}\")\n    \n    # 4. Apply gentle post-processing\n    print(\"Applying gentle post-processing...\")\n    Y_pred_processed = gentle_post_processing(\n        Y_pred_raw,\n        threshold=0.3,\n        min_size=20\n    )\n    \n    # 5. Calculate metrics\n    print(\"Calculating metrics...\")\n    metrics_list = []\n    for i in range(6):\n        metrics = calculate_metrics(\n            Y_true[i].squeeze(),\n            Y_pred_processed[i]\n        )\n        metrics_list.append(metrics)\n        print(f\"Time step {i+12}: Dice = {metrics['dice']:.4f}, IoU = {metrics['iou']:.4f}\")\n    \n    # 6. Visualize results\n    print(\"Creating visualization...\")\n    avg_metrics = visualize_predictions_enhanced(\n        X_sample,\n        Y_true,\n        Y_pred_raw,\n        Y_pred_processed\n    )\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"ANALYSIS COMPLETE - SUMMARY RESULTS\")\n    print(\"=\"*50)\n    for key, value in avg_metrics.items():\n        print(f\"  {key.upper()}: {value:.4f}\")\n    \n    return avg_metrics\n\nprint(\"✅ All analysis functions loaded successfully!\")\nprint(\"Run: analyze_model_results(model, X_val, Y_val) to analyze your results\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T07:51:34.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Plot training history\nplot_training_history(history)\n\n# 2. Choose a validation sample\nsample_idx = 3\nX_sample = X_val[sample_idx]\nY_true = Y_val[sample_idx]\n\nprint(f\"Analyzing sample {sample_idx}...\")\n\n# 3. Get model predictions and fix dtype\nprint(\"Getting model predictions...\")\nY_pred_raw = model.predict(X_sample[np.newaxis, ...])[0]\n\n# CRITICAL FIX: Convert to float32 for scipy compatibility\nY_pred_raw = Y_pred_raw.astype(np.float32)\n\nprint(f\"Prediction shape: {Y_pred_raw.shape}, dtype: {Y_pred_raw.dtype}\")\n\n# 4. Apply post-processing (now works!)\nprint(\"Applying post-processing...\")\nY_pred_processed = gentle_post_processing(\n    Y_pred_raw,\n    threshold=0.3,\n    min_size=20\n)\n\n# 5. Calculate metrics\nprint(\"Calculating metrics...\")\nmetrics_list = []\nfor i in range(6):\n    metrics = calculate_metrics(\n        Y_true[i].squeeze(),\n        Y_pred_processed[i]\n    )\n    metrics_list.append(metrics)\n    print(f\"Time step {i+12}: Dice = {metrics['dice']:.4f}, IoU = {metrics['iou']:.4f}\")\n\n# 6. Visualize results\nprint(\"Creating visualization...\")\navg_metrics = visualize_predictions_enhanced(\n    X_sample,\n    Y_true,\n    Y_pred_raw,\n    Y_pred_processed\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANALYSIS COMPLETE - SUMMARY RESULTS\")\nprint(\"=\"*50)\nfor key, value in avg_metrics.items():\n    print(f\"  {key.upper()}: {value:.4f}\")\n","metadata":{"trusted":true,"id":"0IDhUhwSwuHm","outputId":"07085b41-3f9f-42cc-a2ed-d58c51bede6c","execution":{"execution_failed":"2025-06-16T07:51:34.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nfrom scipy.ndimage import gaussian_filter, binary_erosion, binary_dilation\nfrom skimage import morphology, measure\nimport tensorflow as tf\n\ndef plot_training_history(history):\n    \"\"\"Enhanced training history plotting\"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Loss\n    ax1.plot(history.history['loss'], label='Train Loss', linewidth=2)\n    ax1.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title(\"Training vs Validation Loss\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Dice Score\n    ax2.plot(history.history['dice_coefficient'], label='Train Dice', linewidth=2)\n    ax2.plot(history.history['val_dice_coefficient'], label='Val Dice', linewidth=2)\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Dice Coefficient')\n    ax2.set_title('Dice Score (Train vs Val)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # Learning Rate\n    if 'lr' in history.history:\n        ax3.plot(history.history['lr'], linewidth=2)\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('Learning Rate')\n        ax3.set_title('Learning Rate Schedule')\n        ax3.set_yscale('log')\n        ax3.grid(True, alpha=0.3)\n\n    # Overfitting Analysis\n    train_loss = np.array(history.history['loss'])\n    val_loss = np.array(history.history['val_loss'])\n    overfitting_metric = val_loss - train_loss\n\n    ax4.plot(overfitting_metric, linewidth=2, color='red')\n    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Val Loss - Train Loss')\n    ax4.set_title('Overfitting Indicator')\n    ax4.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\ndef advanced_post_processing(pred, sigma=0.8, threshold=0.5, min_size=50):\n    \"\"\"Advanced post-processing with morphological operations\"\"\"\n    pred_squeezed = pred.squeeze()\n    out = np.zeros_like(pred_squeezed)\n\n    for t in range(pred_squeezed.shape[0]):\n        # Gaussian smoothing\n        smoothed = gaussian_filter(pred_squeezed[t], sigma=sigma)\n\n        # Adaptive thresholding\n        binary = smoothed > threshold\n\n        # Remove small objects\n        binary = morphology.remove_small_objects(binary, min_size=min_size)\n\n        # Fill holes\n        binary = morphology.remove_small_holes(binary, area_threshold=min_size//2)\n\n        # Morphological operations for cleaner boundaries\n        kernel = morphology.disk(2)\n        binary = morphology.opening(binary, kernel)\n        binary = morphology.closing(binary, kernel)\n\n        out[t] = binary.astype(np.uint8)\n\n    return out\n\ndef calculate_metrics(y_true, y_pred):\n    \"\"\"Calculate comprehensive metrics\"\"\"\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n\n    # Basic metrics\n    intersection = np.sum(y_true_f * y_pred_f)\n    union = np.sum(y_true_f) + np.sum(y_pred_f) - intersection\n\n    dice = (2. * intersection) / (np.sum(y_true_f) + np.sum(y_pred_f) + 1e-6)\n    iou = intersection / (union + 1e-6)\n\n    # Precision and Recall\n    tp = intersection\n    fp = np.sum(y_pred_f) - intersection\n    fn = np.sum(y_true_f) - intersection\n\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n\n    return {\n        'dice': dice,\n        'iou': iou,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef visualize_predictions_enhanced(X_sample, Y_true, Y_pred_raw, Y_pred_processed):\n    \"\"\"Enhanced visualization with metrics\"\"\"\n    fig, axs = plt.subplots(4, 6, figsize=(20, 12))\n\n    metrics_list = []\n\n    for i in range(6):\n        # Input frames\n        axs[0, i].imshow(X_sample[i+6].squeeze(), cmap='gray')\n        axs[0, i].set_title(f\"Input t{i+6}\")\n        axs[0, i].axis('off')\n\n        # Raw predictions\n        axs[1, i].imshow(Y_pred_raw[i].squeeze(), cmap='viridis', vmin=0, vmax=1)\n        axs[1, i].set_title(f\"Raw Pred t{i+12}\")\n        axs[1, i].axis('off')\n\n        # Processed predictions\n        axs[2, i].imshow(Y_pred_processed[i], cmap='gray')\n        axs[2, i].set_title(f\"Processed t{i+12}\")\n        axs[2, i].axis('off')\n\n        # Ground truth\n        axs[3, i].imshow(Y_true[i].squeeze(), cmap='gray')\n        axs[3, i].set_title(f\"Ground Truth t{i+12}\")\n        axs[3, i].axis('off')\n\n        # Calculate metrics for this frame\n        metrics = calculate_metrics(Y_true[i].squeeze(), Y_pred_processed[i])\n        metrics_list.append(metrics)\n\n    plt.suptitle(\"Enhanced Cloud Prediction Analysis\", fontsize=16)\n    plt.tight_layout()\n\n    # Print average metrics\n    avg_metrics = {}\n    for key in metrics_list[0].keys():\n        avg_metrics[key] = np.mean([m[key] for m in metrics_list])\n\n    print(\"Average Metrics:\")\n    for key, value in avg_metrics.items():\n        print(f\"{key.upper()}: {value:.4f}\")\n\n    plt.show()\n\n    return avg_metrics\n\n# Data Augmentation Class\nclass DataAugmentation:\n    def __init__(self, rotation_range=10, zoom_range=0.1, horizontal_flip=True):\n        self.rotation_range = rotation_range\n        self.zoom_range = zoom_range\n        self.horizontal_flip = horizontal_flip\n\n    def augment_batch(self, X_batch, Y_batch):\n        \"\"\"Apply augmentation to a batch\"\"\"\n        augmented_X = []\n        augmented_Y = []\n\n        for i in range(len(X_batch)):\n            # Random rotation\n            if np.random.random() < 0.5:\n                angle = np.random.uniform(-self.rotation_range, self.rotation_range)\n                X_aug = self.rotate_3d(X_batch[i], angle)\n                Y_aug = self.rotate_3d(Y_batch[i], angle)\n            else:\n                X_aug, Y_aug = X_batch[i], Y_batch[i]\n\n            # Random horizontal flip\n            if self.horizontal_flip and np.random.random() < 0.5:\n                X_aug = np.flip(X_aug, axis=2)\n                Y_aug = np.flip(Y_aug, axis=2)\n\n            augmented_X.append(X_aug)\n            augmented_Y.append(Y_aug)\n\n        return np.array(augmented_X), np.array(augmented_Y)\n\n    def rotate_3d(self, volume, angle):\n        \"\"\"Rotate 3D volume\"\"\"\n        from scipy.ndimage import rotate\n        return rotate(volume, angle, axes=(1, 2), reshape=False, mode='nearest')\n","metadata":{"trusted":true,"id":"W0HEDx8bwuHm","execution":{"iopub.status.busy":"2025-06-16T05:49:24.796639Z","iopub.status.idle":"2025-06-16T05:49:24.796873Z","shell.execute_reply.started":"2025-06-16T05:49:24.796764Z","shell.execute_reply":"2025-06-16T05:49:24.796774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Then all your analysis functions\nplot_training_history(history)  # Analyze how training went\nadvanced_post_processing()      # Process predictions\ncalculate_metrics()             # Evaluate performance\nvisualize_predictions_enhanced() # Show results","metadata":{"trusted":true,"id":"5D_XuOuEwuHm","outputId":"0bf1dc0c-dd4a-4df5-ea5d-415722697d76","execution":{"iopub.status.busy":"2025-06-16T05:49:24.798182Z","iopub.status.idle":"2025-06-16T05:49:24.798519Z","shell.execute_reply.started":"2025-06-16T05:49:24.798348Z","shell.execute_reply":"2025-06-16T05:49:24.798364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"h5Ac43TGwuHm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"WXzJvFJCr5Lk","trusted":true},"outputs":[],"execution_count":null}]}